{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "import re \n",
    "from os import listdir \n",
    "from numpy import array \n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from keras.utils.vis_utils import plot_model \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "from keras.layers import Flatten \n",
    "from keras.layers import Embedding \n",
    "from keras.layers.convolutional import Conv1D \n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    f = open(filename, 'r')\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs(directory,vocab):\n",
    "    for fn in listdir(directory):\n",
    "        if not fn.endswith('.txt'):\n",
    "            next\n",
    "        path = directory + '/' + fn\n",
    "        add_doc_to_vocab(path, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs_after_vocab(directory, vocab, istrain):\n",
    "    lines=list()\n",
    "    for fn in listdir(directory):\n",
    "        if istrain and fn.startswith('cv9' ):\n",
    "            continue\n",
    "        if not istrain and not fn.startswith('cv9' ):\n",
    "            continue\n",
    "        path = directory + '/' + fn\n",
    "        line = doc_to_line(path, vocab)\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_line(fn, vocab):\n",
    "    doc = load_doc(fn)\n",
    "    tokens = clean_doc(doc)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    tokens = doc.split()\n",
    "    re_punc = re.compile(' [%s]' % re.escape(string.punctuation))\n",
    "    tokens = [re_punc.sub(' ' , w) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_doc_to_vocab(fn, vocab):\n",
    "    doc = load_doc(fn)\n",
    "    tokens  = clean_doc(doc)\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(lines, fn):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(fn,'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=Counter()\n",
    "process_docs('txt_sentoken/neg' , vocab) \n",
    "process_docs('txt_sentoken/pos' , vocab)\n",
    "min_occurance = 5\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurance] \n",
    "save_list(tokens, 'vocab.txt')\n",
    "vocab = load_doc('vocab.txt')\n",
    "vocab = vocab.split() \n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_dataset(vocab, istrain):\n",
    "    neg = process_docs_after_vocab('txt_sentoken/neg' , vocab, istrain)\n",
    "    pos = process_docs_after_vocab('txt_sentoken/pos' , vocab, istrain) \n",
    "    docs = neg+pos\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(neg))]\n",
    "    return docs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokens(lines):\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(lines)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "print(len(ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = create_tokens(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13850"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(s.split()) for s in train_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_docs(tokenizer, max_length, docs):\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post' )\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = encode_docs(tokenizer, max_length, train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1186, 100)         1385000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1179, 32)          25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 589, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18848)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                188490    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,599,133\n",
      "Trainable params: 1,599,133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = define_model(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1800/1800 [==============================] - 14s 8ms/step - loss: 0.6895 - accuracy: 0.5367\n",
      "Epoch 2/10\n",
      "1800/1800 [==============================] - 14s 8ms/step - loss: 0.4489 - accuracy: 0.8339\n",
      "Epoch 3/10\n",
      "1800/1800 [==============================] - 12s 7ms/step - loss: 0.0533 - accuracy: 0.9900\n",
      "Epoch 4/10\n",
      "1800/1800 [==============================] - 12s 7ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "1800/1800 [==============================] - 13s 7ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "1800/1800 [==============================] - 12s 7ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "1800/1800 [==============================] - 12s 7ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1800/1800 [==============================] - 12s 7ms/step - loss: 9.4750e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1800/1800 [==============================] - 13s 7ms/step - loss: 7.8859e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1800/1800 [==============================] - 12s 7ms/step - loss: 6.5325e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x22cc5390f88>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xtrain, ytrain, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = encode_docs(tokenizer, max_length, test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, acc = model.evaluate(Xtest, ytest, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
